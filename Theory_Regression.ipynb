{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxPeOwfoPZRy1BfvDZnaR/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alibagheribardi/Regression/blob/main/Theory_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let $X$ be a numerical dataset comprising $N$ observations, where each observation is characterized by $m$ numerical features, and let $y$ represent the corresponding target variable. The objective of Ordinary Least Squares (OLS) regression is to determine the coefficient vector $\\beta$ that minimizes the residual error $\\|X\\beta - y\\|$. Geometrically, the optimal coefficient vector $\\beta$ is obtained by projecting $y$ onto the column space of $X$, ensuring that the residual vector $y - X\\beta$ is orthogonal to this subspace. It is mathematically formulated by  \n",
        "\n",
        "## $$X^T(X\\beta - y) = 0,$$  \n",
        "\n",
        "## which yields  \n",
        "\n",
        "## $$\\beta = (X^T X)^{-1} X^T y.$$\n",
        "\n",
        "\n",
        "##Remarks.\n",
        "\n",
        "##- In this approach, the Euclidean norm defines the notion of distance. However, various other distance metrics are commonly employed in machine learning algorithms, which may be more suitable in certain cases. These metrics are not induced by an inner product, and consequently, the concept of orthogonality does not necessarily hold. This absence poses significant challenges when formulating the coefficient vector $\\beta$.\n",
        "    \n",
        "## Although the square matrix $X^T X$ is generally nonsingular, numerical instability can pose significant challenges, potentially compromising computational reliability. To mitigate this issue, various regularization techniques, such as Lasso and Ridge regression, are employed to enhance numerical stability and prevent ill-conditioning.  \n",
        "\n",
        "## As a methodological refinement, one may introduce a controlled perturbation to the original dataset $X$ by defining $X_{\\lambda} = X + \\lambda I'$, where $\\lambda$ is a nonzero scalar. Ordinary least squares (OLS) estimation is then applied to this modified dataset, with the corresponding loss function given by:\n",
        "\n",
        "\n",
        "##$$\n",
        "\\tilde{L}(\\beta, \\lambda) = \\| (X \\beta - y)    + \\lambda I'\\beta\\|^2.\n",
        "$$\n",
        "\n",
        "## Geometrically, this corresponds to projecting the target vector $y$ onto the hyperplane spanned by the columns of $X + \\lambda I'$.\n",
        "\n",
        "## Sufficiently small scalars $\\lambda$ that ensure the numerical stability of the inverse of $X_{\\lambda}^T X_{\\lambda}$ lead to the following prediction:  \n",
        "\n",
        "## $$\n",
        "y_{\\text{pred}} = X_\\lambda\\beta  ~~~~\\text{where}~~~\\beta = (X_\\lambda^T X_\\lambda)^{-1} X_\\lambda^Ty\n",
        "$$\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Hgqzc0-tzF5K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applying SVD in Rgression\n"
      ],
      "metadata": {
        "id": "qT-1oFYEyP_O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Singular Value Decomposotion Theorem**:  Let $X$ be any $n \\times p$ matrix.  Then we can find a factorization\n",
        "\n",
        "$$X = U \\Sigma V^\\top$$\n",
        "\n",
        "where\n",
        "\n",
        "* $V$ is an orthogonal $p \\times p$ matrix (its columns form an orthonormal basis of $\\mathbb{R}^p$).\n",
        "* $U$ is an orthogonal $n \\times n$ matrix (its columns form an orthonormal basis of $\\mathbb{R}^n$).\n",
        "* $\\Sigma$ is a $n \\times p$ matrix which has zero entries everywhere but the diagonal $\\Sigma_{jj} = \\sigma_j \\geq 0$.\n",
        "\n",
        "Here we only take the $r = \\textrm{Rank}(X)$ left and right singular vectors with non-zero singular values.  So we get the decomposition\n",
        "\n",
        "$$X = U_r \\Sigma_r V_r^\\top$$\n",
        "\n",
        "where $V_r$ is an $p \\times r$ matrix, $\\Sigma_r$ is an $r \\times r$ diagonal matrix with positive values along the diagonal, and $U_r$ is an $n \\times r$ matrix.\n",
        "\n",
        "The columns of $U_r$ form an orthonormal basis of the image of $X$.  So it is easy to use it to project a vector $\\vec{y}$ onto the image of $X$:\n",
        "\n",
        "$$\\hat{y} = \\text{Proj}_{\\textrm{Im}(X)} (\\vec{y}) = \\sum_1^r (\\vec{y} \\cdot \\vec{u}_j) \\vec{u_j}$$\n",
        "\n",
        "To find the value of $\\beta$ with $\\hat{y} = X \\vec{\\beta}$, we can use\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "X \\beta &= U_{r} U_{r}^\\top \\vec{y}\\\\\n",
        "U_{r} \\Sigma_r V_r^\\top \\beta &= U_{r} U_{r}^\\top \\vec{y}\\\\\n",
        "\\Sigma_r V_r^\\top \\beta &= U_{r}^\\top \\vec{y}\\\\\n",
        "V_r^\\top \\beta  &=  \\Sigma_r^{-1} U_{r}^\\top \\vec{y}\\\\\n",
        "\\beta &= V_r \\Sigma_r^{-1} U_{r}^\\top \\vec{y}\n",
        "\\end{align*}\n",
        "$$"
      ],
      "metadata": {
        "id": "F4vnliS_vxsS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cEhEDgwtv037"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}